{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4cfb25a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46559b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets torch scikit-learn pandas numpy accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fea49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "y:\\07\\DeepLearning\\megatron\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Memory: 8.59 GB\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102c383",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b313cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtask 1: Binary Classification\n",
      "Training samples: 3222\n",
      "Test samples: 160\n",
      "Class distribution:\n",
      "polarization\n",
      "0    2047\n",
      "1    1175\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Subtask 2: Multi-label Classification\n",
      "Training samples: 3222\n",
      "Test samples: 160\n",
      "\n",
      "Label distribution:\n",
      "  political: 1150 (35.69%)\n",
      "  racial/ethnic: 281 (8.72%)\n",
      "  religious: 112 (3.48%)\n",
      "  gender/sexual: 72 (2.23%)\n",
      "  other: 126 (3.91%)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_train_binary = pd.read_csv('New_train_subtask1_eng.csv')\n",
    "df_test = pd.read_csv('New_dev_subtask1_eng.csv')\n",
    "df_train_multilabel = pd.read_csv('New_train_subtask2_eng.csv')\n",
    "df_test_multilabel = pd.read_csv('New_dev_subtask2_eng.csv')\n",
    "\n",
    "label_columns = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']\n",
    "\n",
    "print(\"Subtask 1: Binary Classification\")\n",
    "print(f\"Training samples: {len(df_train_binary)}\")\n",
    "print(f\"Test samples: {len(df_test)}\")\n",
    "print(f\"Class distribution:\\n{df_train_binary['polarization'].value_counts()}\")\n",
    "\n",
    "print(\"\\nSubtask 2: Multi-label Classification\")\n",
    "print(f\"Training samples: {len(df_train_multilabel)}\")\n",
    "print(f\"Test samples: {len(df_test_multilabel)}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "for col in label_columns:\n",
    "    count = df_train_multilabel[col].sum()\n",
    "    pct = df_train_multilabel[col].mean() * 100\n",
    "    print(f\"  {col}: {count} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d3594",
   "metadata": {},
   "source": [
    "## 3. Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08657ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarizationDataset(Dataset):\n",
    "    \"\"\"Custom dataset for binary classification.\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    \"\"\"Custom dataset for multi-label classification.\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653f014",
   "metadata": {},
   "source": [
    "## 4. Subtask 1: Binary Polarization Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a82c9",
   "metadata": {},
   "source": [
    "### 4.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b85d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2738\n",
      "Validation samples: 484\n",
      "Train class distribution: [1740  998]\n",
      "Validation class distribution: [307 177]\n"
     ]
    }
   ],
   "source": [
    "# Train-validation split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_train_binary['text'].values,\n",
    "    df_train_binary['polarization'].values,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=df_train_binary['polarization'].values\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Train class distribution: {np.bincount(train_labels)}\")\n",
    "print(f\"Validation class distribution: {np.bincount(val_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02997941",
   "metadata": {},
   "source": [
    "### 4.2 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "472feeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "Parameters: 124,647,170\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer_binary = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_binary = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "train_dataset = PolarizationDataset(train_texts, train_labels, tokenizer_binary)\n",
    "val_dataset = PolarizationDataset(val_texts, val_labels, tokenizer_binary)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {model_binary.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01263e32",
   "metadata": {},
   "source": [
    "### 4.3 Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e43d5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_binary(eval_pred):\n",
    "    \"\"\"Calculate F1-score and accuracy for binary classification.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "    \n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_class_0': f1_per_class[0],\n",
    "        'f1_class_1': f1_per_class[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639fe97",
   "metadata": {},
   "source": [
    "### 4.4 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544923c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_binary = TrainingArguments(\n",
    "    output_dir='./results_binary',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_binary',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type='linear',\n",
    ")\n",
    "\n",
    "trainer_binary = Trainer(\n",
    "    model=model_binary,\n",
    "    args=training_args_binary,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics_binary,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bea3e",
   "metadata": {},
   "source": [
    "### 4.5 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a5287d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training binary classification model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='700' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [700/860 02:29 < 00:34, 4.67 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.475100</td>\n",
       "      <td>0.421706</td>\n",
       "      <td>0.777976</td>\n",
       "      <td>0.799587</td>\n",
       "      <td>0.847244</td>\n",
       "      <td>0.708709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.419100</td>\n",
       "      <td>0.420355</td>\n",
       "      <td>0.809381</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.850340</td>\n",
       "      <td>0.768421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.428992</td>\n",
       "      <td>0.795505</td>\n",
       "      <td>0.803719</td>\n",
       "      <td>0.836489</td>\n",
       "      <td>0.754522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.547783</td>\n",
       "      <td>0.829940</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.877023</td>\n",
       "      <td>0.782857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>0.474612</td>\n",
       "      <td>0.821859</td>\n",
       "      <td>0.834711</td>\n",
       "      <td>0.869707</td>\n",
       "      <td>0.774011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.797985</td>\n",
       "      <td>0.812577</td>\n",
       "      <td>0.830579</td>\n",
       "      <td>0.870662</td>\n",
       "      <td>0.754491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.716040</td>\n",
       "      <td>0.826275</td>\n",
       "      <td>0.836777</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.783562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training binary classification model...\")\n",
    "trainer_binary.train()\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601bdae",
   "metadata": {},
   "source": [
    "### 4.6 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd1f662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtask 1 - Binary Classification Results\n",
      "Macro F1: 0.8299\n",
      "Accuracy: 0.8430\n",
      "F1 (Non-Polarized): 0.8770\n",
      "F1 (Polarized): 0.7829\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Polarized       0.87      0.88      0.88       307\n",
      "    Polarized       0.79      0.77      0.78       177\n",
      "\n",
      "     accuracy                           0.84       484\n",
      "    macro avg       0.83      0.83      0.83       484\n",
      " weighted avg       0.84      0.84      0.84       484\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer_binary.evaluate()\n",
    "\n",
    "print(\"Subtask 1 - Binary Classification Results\")\n",
    "print(f\"Macro F1: {eval_results['eval_macro_f1']:.4f}\")\n",
    "print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 (Non-Polarized): {eval_results['eval_f1_class_0']:.4f}\")\n",
    "print(f\"F1 (Polarized): {eval_results['eval_f1_class_1']:.4f}\")\n",
    "\n",
    "predictions = trainer_binary.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(val_labels, pred_labels, target_names=['Non-Polarized', 'Polarized']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a4a15",
   "metadata": {},
   "source": [
    "### 4.7 Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d329825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions: Polarized=52, Non-Polarized=108\n",
      "Saved: submission_subtask1.csv\n"
     ]
    }
   ],
   "source": [
    "test_texts = df_test['text'].values\n",
    "test_dataset = PolarizationDataset(\n",
    "    test_texts, \n",
    "    np.zeros(len(test_texts)),\n",
    "    tokenizer_binary\n",
    ")\n",
    "\n",
    "test_predictions = trainer_binary.predict(test_dataset)\n",
    "test_pred_labels = np.argmax(test_predictions.predictions, axis=1)\n",
    "\n",
    "df_submission_task1 = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'polarization': test_pred_labels\n",
    "})\n",
    "df_submission_task1.to_csv('submission_subtask1.csv', index=False)\n",
    "\n",
    "print(f\"Test predictions: Polarized={test_pred_labels.sum()}, Non-Polarized={len(test_pred_labels) - test_pred_labels.sum()}\")\n",
    "print(\"Saved: submission_subtask1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48864e7f",
   "metadata": {},
   "source": [
    "## 5. Subtask 2: Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f032d2",
   "metadata": {},
   "source": [
    "### 5.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608b4150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2738\n",
      "Validation samples: 484\n",
      "\n",
      "Label distribution:\n",
      "  political: 1003 (36.63%)\n",
      "  racial/ethnic: 244 (8.91%)\n",
      "  religious: 98 (3.58%)\n",
      "  gender/sexual: 63 (2.30%)\n",
      "  other: 104 (3.80%)\n"
     ]
    }
   ],
   "source": [
    "train_texts_ml, val_texts_ml, train_labels_ml, val_labels_ml = train_test_split(\n",
    "    df_train_multilabel['text'].values,\n",
    "    df_train_multilabel[label_columns].values,\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts_ml)}\")\n",
    "print(f\"Validation samples: {len(val_texts_ml)}\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "for i, col in enumerate(label_columns):\n",
    "    count = train_labels_ml[:, i].sum()\n",
    "    pct = train_labels_ml[:, i].mean() * 100\n",
    "    print(f\"  {col}: {count} ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba1be7",
   "metadata": {},
   "source": [
    "### 5.2 Custom Trainer for Multi-Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c13d5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"Trainer with BCE loss for multi-label classification.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69f07d",
   "metadata": {},
   "source": [
    "### 5.3 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c8d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label model initialized with 124,649,477 parameters\n"
     ]
    }
   ],
   "source": [
    "tokenizer_multilabel = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_multilabel = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=5,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "train_dataset_ml = MultiLabelDataset(train_texts_ml, train_labels_ml, tokenizer_multilabel)\n",
    "val_dataset_ml = MultiLabelDataset(val_texts_ml, val_labels_ml, tokenizer_multilabel)\n",
    "\n",
    "print(f\"Multi-label model initialized with {model_multilabel.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef4bfd",
   "metadata": {},
   "source": [
    "### 5.4 Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e6774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_multilabel(eval_pred):\n",
    "    \"\"\"Calculate macro F1 and accuracy for multi-label classification.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "    predictions = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    per_label_f1 = f1_score(labels, predictions, average=None, zero_division=0)\n",
    "    \n",
    "    results = {'macro_f1': macro_f1, 'accuracy': accuracy}\n",
    "    for i, col in enumerate(label_columns):\n",
    "        results[f'f1_{col}'] = per_label_f1[i]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd63a6",
   "metadata": {},
   "source": [
    "### 5.5 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e64805f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_multilabel = TrainingArguments(\n",
    "    output_dir='./results_multilabel',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_multilabel',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type='linear',\n",
    ")\n",
    "\n",
    "trainer_multilabel = MultiLabelTrainer(\n",
    "    model=model_multilabel,\n",
    "    args=training_args_multilabel,\n",
    "    train_dataset=train_dataset_ml,\n",
    "    eval_dataset=val_dataset_ml,\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fb03d",
   "metadata": {},
   "source": [
    "### 5.6 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0f3e070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multi-label classification model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='860' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [860/860 02:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Political</th>\n",
       "      <th>F1 Racial/ethnic</th>\n",
       "      <th>F1 Religious</th>\n",
       "      <th>F1 Gender/sexual</th>\n",
       "      <th>F1 Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.237710</td>\n",
       "      <td>0.100448</td>\n",
       "      <td>0.721074</td>\n",
       "      <td>0.502242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.227200</td>\n",
       "      <td>0.200266</td>\n",
       "      <td>0.141754</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.708772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>0.198230</td>\n",
       "      <td>0.158401</td>\n",
       "      <td>0.702479</td>\n",
       "      <td>0.689441</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.208122</td>\n",
       "      <td>0.220688</td>\n",
       "      <td>0.663223</td>\n",
       "      <td>0.688347</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.162300</td>\n",
       "      <td>0.197380</td>\n",
       "      <td>0.203284</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.712074</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.205276</td>\n",
       "      <td>0.257629</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.685535</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.211229</td>\n",
       "      <td>0.324759</td>\n",
       "      <td>0.716942</td>\n",
       "      <td>0.675410</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.209652</td>\n",
       "      <td>0.354637</td>\n",
       "      <td>0.716942</td>\n",
       "      <td>0.682274</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training multi-label classification model...\")\n",
    "trainer_multilabel.train()\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c2b99",
   "metadata": {},
   "source": [
    "### 5.7 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f45916e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtask 2 - Multi-Label Classification Results (Baseline)\n",
      "Macro F1: 0.3546\n",
      "Accuracy: 0.7169\n",
      "\n",
      "Per-label F1 scores:\n",
      "  political: 0.6823\n",
      "  racial/ethnic: 0.5455\n",
      "  religious: 0.5455\n",
      "  gender/sexual: 0.0000\n",
      "  other: 0.0000\n"
     ]
    }
   ],
   "source": [
    "eval_results_ml = trainer_multilabel.evaluate()\n",
    "\n",
    "print(\"Subtask 2 - Multi-Label Classification Results (Baseline)\")\n",
    "print(f\"Macro F1: {eval_results_ml['eval_macro_f1']:.4f}\")\n",
    "print(f\"Accuracy: {eval_results_ml['eval_accuracy']:.4f}\")\n",
    "print(\"\\nPer-label F1 scores:\")\n",
    "for col in label_columns:\n",
    "    print(f\"  {col}: {eval_results_ml[f'eval_f1_{col}']:.4f}\")\n",
    "\n",
    "predictions_ml = trainer_multilabel.predict(val_dataset_ml)\n",
    "pred_probs_ml = torch.sigmoid(torch.tensor(predictions_ml.predictions)).numpy()\n",
    "pred_labels_ml = (pred_probs_ml > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee06a63",
   "metadata": {},
   "source": [
    "## 6. Advanced Optimization: Class-Weighted Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d6489",
   "metadata": {},
   "source": [
    "### 6.1 Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890bf69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights:\n",
      "  political: 1.73\n",
      "  racial/ethnic: 10.22\n",
      "  religious: 26.94\n",
      "  gender/sexual: 42.46\n",
      "  other: 25.33\n"
     ]
    }
   ],
   "source": [
    "pos_weights = []\n",
    "print(\"Class weights:\")\n",
    "for i, col in enumerate(label_columns):\n",
    "    pos_count = train_labels_ml[:, i].sum()\n",
    "    neg_count = len(train_labels_ml) - pos_count\n",
    "    weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    pos_weights.append(weight)\n",
    "    print(f\"  {col}: {weight:.2f}\")\n",
    "\n",
    "pos_weight_tensor = torch.tensor(pos_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4cdf6",
   "metadata": {},
   "source": [
    "### 6.2 Weighted Trainer Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dca8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMultiLabelTrainer(Trainer):\n",
    "    \"\"\"Trainer with weighted BCE loss for imbalanced multi-label classification.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818eaed",
   "metadata": {},
   "source": [
    "### 6.3 Train Weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "562e5564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training class-weighted model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='860' max='860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [860/860 02:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Political</th>\n",
       "      <th>F1 Racial/ethnic</th>\n",
       "      <th>F1 Religious</th>\n",
       "      <th>F1 Gender/sexual</th>\n",
       "      <th>F1 Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.227100</td>\n",
       "      <td>1.042984</td>\n",
       "      <td>0.260128</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.632588</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.136646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.022300</td>\n",
       "      <td>0.885461</td>\n",
       "      <td>0.331778</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.668639</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.208696</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.268041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.821395</td>\n",
       "      <td>0.341454</td>\n",
       "      <td>0.601240</td>\n",
       "      <td>0.710602</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>0.942623</td>\n",
       "      <td>0.377282</td>\n",
       "      <td>0.605372</td>\n",
       "      <td>0.633880</td>\n",
       "      <td>0.485981</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.963006</td>\n",
       "      <td>0.417107</td>\n",
       "      <td>0.654959</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.482900</td>\n",
       "      <td>1.085634</td>\n",
       "      <td>0.400651</td>\n",
       "      <td>0.665289</td>\n",
       "      <td>0.678679</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.536000</td>\n",
       "      <td>1.183499</td>\n",
       "      <td>0.398888</td>\n",
       "      <td>0.683884</td>\n",
       "      <td>0.680982</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>1.266889</td>\n",
       "      <td>0.424661</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "model_multilabel_weighted = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=5,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "trainer_weighted = WeightedMultiLabelTrainer(\n",
    "    model=model_multilabel_weighted,\n",
    "    args=training_args_multilabel,\n",
    "    train_dataset=train_dataset_ml,\n",
    "    eval_dataset=val_dataset_ml,\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"Training class-weighted model...\")\n",
    "trainer_weighted.train()\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1dbf1",
   "metadata": {},
   "source": [
    "### 6.4 Evaluate Weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b97b2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtask 2 - Class-Weighted Model Results\n",
      "Macro F1: 0.4247\n",
      "Accuracy: 0.6860\n",
      "Improvement over baseline: 0.0700\n"
     ]
    }
   ],
   "source": [
    "eval_results_weighted = trainer_weighted.evaluate()\n",
    "\n",
    "print(\"Subtask 2 - Class-Weighted Model Results\")\n",
    "print(f\"Macro F1: {eval_results_weighted['eval_macro_f1']:.4f}\")\n",
    "print(f\"Accuracy: {eval_results_weighted['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement over baseline: {(eval_results_weighted['eval_macro_f1'] - eval_results_ml['eval_macro_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb164ae",
   "metadata": {},
   "source": [
    "## 7. Advanced Optimization: Threshold Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d893e",
   "metadata": {},
   "source": [
    "### 7.1 Find Optimal Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1311289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal thresholds for baseline model:\n",
      "\n",
      "political: threshold=0.10, F1=0.6994\n",
      "racial/ethnic: threshold=0.35, F1=0.5897\n",
      "religious: threshold=0.50, F1=0.5455\n",
      "gender/sexual: threshold=0.10, F1=0.1622\n",
      "other: threshold=0.15, F1=0.2000\n",
      "\n",
      "Baseline + Threshold Tuning Results\n",
      "Macro F1: 0.4394\n",
      "Accuracy: 0.6674\n",
      "Improvement: 0.0847\n"
     ]
    }
   ],
   "source": [
    "def find_best_thresholds(probabilities, true_labels, label_names):\n",
    "    \"\"\"Find optimal decision threshold for each label to maximize F1.\"\"\"\n",
    "    best_thresholds = []\n",
    "    \n",
    "    for i, label_name in enumerate(label_names):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            preds = (probabilities[:, i] > threshold).astype(int)\n",
    "            f1 = f1_score(true_labels[:, i], preds, zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        best_thresholds.append(best_threshold)\n",
    "        print(f\"{label_name}: threshold={best_threshold:.2f}, F1={best_f1:.4f}\")\n",
    "    \n",
    "    return best_thresholds\n",
    "\n",
    "print(\"Finding optimal thresholds for baseline model:\\n\")\n",
    "optimal_thresholds = find_best_thresholds(pred_probs_ml, val_labels_ml, label_columns)\n",
    "\n",
    "pred_labels_optimized = np.zeros_like(pred_probs_ml, dtype=int)\n",
    "for i, threshold in enumerate(optimal_thresholds):\n",
    "    pred_labels_optimized[:, i] = (pred_probs_ml[:, i] > threshold).astype(int)\n",
    "\n",
    "macro_f1_optimized = f1_score(val_labels_ml, pred_labels_optimized, average='macro', zero_division=0)\n",
    "accuracy_optimized = accuracy_score(val_labels_ml, pred_labels_optimized)\n",
    "\n",
    "print(f\"\\nBaseline + Threshold Tuning Results\")\n",
    "print(f\"Macro F1: {macro_f1_optimized:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_optimized:.4f}\")\n",
    "print(f\"Improvement: {(macro_f1_optimized - eval_results_ml['eval_macro_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245869e",
   "metadata": {},
   "source": [
    "### 7.2 Threshold Tuning on Weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b6c7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal thresholds for weighted model:\n",
      "\n",
      "political: threshold=0.45, F1=0.6901\n",
      "racial/ethnic: threshold=0.60, F1=0.5753\n",
      "religious: threshold=0.85, F1=0.6667\n",
      "gender/sexual: threshold=0.85, F1=0.3000\n",
      "other: threshold=0.15, F1=0.2368\n",
      "\n",
      "Class-Weighted + Threshold Tuning Results\n",
      "Macro F1: 0.4938\n",
      "Accuracy: 0.6860\n",
      "Improvement over baseline: 0.1391\n"
     ]
    }
   ],
   "source": [
    "predictions_weighted = trainer_weighted.predict(val_dataset_ml)\n",
    "pred_probs_weighted = torch.sigmoid(torch.tensor(predictions_weighted.predictions)).numpy()\n",
    "\n",
    "print(\"Finding optimal thresholds for weighted model:\\n\")\n",
    "optimal_thresholds_weighted = find_best_thresholds(pred_probs_weighted, val_labels_ml, label_columns)\n",
    "\n",
    "pred_labels_weighted_optimized = np.zeros_like(pred_probs_weighted, dtype=int)\n",
    "for i, threshold in enumerate(optimal_thresholds_weighted):\n",
    "    pred_labels_weighted_optimized[:, i] = (pred_probs_weighted[:, i] > threshold).astype(int)\n",
    "\n",
    "macro_f1_weighted_optimized = f1_score(val_labels_ml, pred_labels_weighted_optimized, average='macro', zero_division=0)\n",
    "accuracy_weighted_optimized = accuracy_score(val_labels_ml, pred_labels_weighted_optimized)\n",
    "\n",
    "print(f\"\\nClass-Weighted + Threshold Tuning Results\")\n",
    "print(f\"Macro F1: {macro_f1_weighted_optimized:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_weighted_optimized:.4f}\")\n",
    "print(f\"Improvement over baseline: {(macro_f1_weighted_optimized - eval_results_ml['eval_macro_f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede55c6",
   "metadata": {},
   "source": [
    "## 8. Advanced Approach: Binary Ensemble\n",
    "\n",
    "Train 5 separate binary classifiers (one per label) with class weighting. This approach often achieves the best performance for imbalanced multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d1103e",
   "metadata": {},
   "source": [
    "### 8.1 Binary Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18cb5075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Binary Ensemble: 5 separate classifiers\n",
      "================================================================================\n",
      "\n",
      "Classifier 1/5: political\n",
      "------------------------------------------------------------\n",
      "Class distribution: Positive=1003, Negative=1735\n",
      "Class weight: 1.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='688' max='688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [688/688 02:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.474770</td>\n",
       "      <td>0.760751</td>\n",
       "      <td>0.799587</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.664360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.372700</td>\n",
       "      <td>0.469932</td>\n",
       "      <td>0.772745</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.844584</td>\n",
       "      <td>0.700906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>0.706694</td>\n",
       "      <td>0.752740</td>\n",
       "      <td>0.789256</td>\n",
       "      <td>0.847761</td>\n",
       "      <td>0.657718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.904352</td>\n",
       "      <td>0.752589</td>\n",
       "      <td>0.787190</td>\n",
       "      <td>0.845113</td>\n",
       "      <td>0.660066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.7727, Accuracy: 0.7955\n",
      "Optimal threshold: 0.35 (F1: 0.7099)\n",
      "\n",
      "Classifier 2/5: racial/ethnic\n",
      "------------------------------------------------------------\n",
      "Class distribution: Positive=244, Negative=2494\n",
      "Class weight: 10.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='688' max='688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [688/688 23:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.394509</td>\n",
       "      <td>0.653168</td>\n",
       "      <td>0.822314</td>\n",
       "      <td>0.895377</td>\n",
       "      <td>0.410959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.469471</td>\n",
       "      <td>0.741099</td>\n",
       "      <td>0.915289</td>\n",
       "      <td>0.953462</td>\n",
       "      <td>0.528736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.893967</td>\n",
       "      <td>0.751255</td>\n",
       "      <td>0.929752</td>\n",
       "      <td>0.961969</td>\n",
       "      <td>0.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>1.156960</td>\n",
       "      <td>0.738212</td>\n",
       "      <td>0.929752</td>\n",
       "      <td>0.962138</td>\n",
       "      <td>0.514286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.7513, Accuracy: 0.9298\n",
      "Optimal threshold: 0.65 (F1: 0.5479)\n",
      "\n",
      "Classifier 3/5: religious\n",
      "------------------------------------------------------------\n",
      "Class distribution: Positive=98, Negative=2640\n",
      "Class weight: 26.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='688' max='688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [688/688 34:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.087000</td>\n",
       "      <td>1.147368</td>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.985325</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.039700</td>\n",
       "      <td>1.028243</td>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.985325</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.950600</td>\n",
       "      <td>0.855510</td>\n",
       "      <td>0.492662</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.985325</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>0.677317</td>\n",
       "      <td>0.665977</td>\n",
       "      <td>0.969008</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.6660, Accuracy: 0.9690\n",
      "Optimal threshold: 0.15 (F1: 0.5556)\n",
      "\n",
      "Classifier 4/5: gender/sexual\n",
      "------------------------------------------------------------\n",
      "Class distribution: Positive=63, Negative=2675\n",
      "Class weight: 42.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='688' max='688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [688/688 33:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.173267</td>\n",
       "      <td>0.495308</td>\n",
       "      <td>0.981405</td>\n",
       "      <td>0.990615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.003400</td>\n",
       "      <td>1.389913</td>\n",
       "      <td>0.495308</td>\n",
       "      <td>0.981405</td>\n",
       "      <td>0.990615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.766400</td>\n",
       "      <td>1.148019</td>\n",
       "      <td>0.642616</td>\n",
       "      <td>0.971074</td>\n",
       "      <td>0.985232</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>1.225041</td>\n",
       "      <td>0.660351</td>\n",
       "      <td>0.975207</td>\n",
       "      <td>0.987368</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.6604, Accuracy: 0.9752\n",
      "Optimal threshold: 0.30 (F1: 0.3810)\n",
      "\n",
      "Classifier 5/5: other\n",
      "------------------------------------------------------------\n",
      "Class distribution: Positive=104, Negative=2634\n",
      "Class weight: 25.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='688' max='688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [688/688 34:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Class 0</th>\n",
       "      <th>F1 Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.798600</td>\n",
       "      <td>0.783996</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.175600</td>\n",
       "      <td>1.248712</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.116400</td>\n",
       "      <td>1.950869</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.734700</td>\n",
       "      <td>1.998301</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.4884, Accuracy: 0.9545\n",
      "Optimal threshold: 0.10 (F1: 0.0870)\n",
      "\n",
      "================================================================================\n",
      "Binary Ensemble Training Complete\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Binary Ensemble: 5 separate classifiers\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ensemble_trainers = []\n",
    "ensemble_models = []\n",
    "ensemble_thresholds = []\n",
    "ensemble_f1_scores = []\n",
    "\n",
    "for label_idx, label_name in enumerate(label_columns):\n",
    "    print(f\"\\nClassifier {label_idx + 1}/5: {label_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Prepare single-label data\n",
    "    train_labels_single = train_labels_ml[:, label_idx]\n",
    "    val_labels_single = val_labels_ml[:, label_idx]\n",
    "    \n",
    "    # Class distribution\n",
    "    pos_count = train_labels_single.sum()\n",
    "    neg_count = len(train_labels_single) - pos_count\n",
    "    class_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "    \n",
    "    print(f\"Class distribution: Positive={pos_count}, Negative={neg_count}\")\n",
    "    print(f\"Class weight: {class_weight:.2f}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset_single = PolarizationDataset(train_texts_ml, train_labels_single, tokenizer_multilabel)\n",
    "    val_dataset_single = PolarizationDataset(val_texts_ml, val_labels_single, tokenizer_multilabel)\n",
    "    \n",
    "    # Initialize model\n",
    "    model_single = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Weighted trainer\n",
    "    class WeightedBinaryTrainer(Trainer):\n",
    "        def __init__(self, *args, weight=1.0, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.weight = torch.tensor([weight], dtype=torch.float).to(device)\n",
    "        \n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.pop(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, self.weight.item()]).to(device))\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    # Training arguments\n",
    "    args_single = TrainingArguments(\n",
    "        output_dir=f'./results_ensemble_{label_name.replace(\"/\", \"_\")}',\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='macro_f1',\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        learning_rate=2e-5,\n",
    "        save_total_limit=1,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer_single = WeightedBinaryTrainer(\n",
    "        model=model_single,\n",
    "        args=args_single,\n",
    "        train_dataset=train_dataset_single,\n",
    "        eval_dataset=val_dataset_single,\n",
    "        compute_metrics=compute_metrics_binary,\n",
    "        weight=class_weight\n",
    "    )\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    trainer_single.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    results = trainer_single.evaluate()\n",
    "    val_f1 = results['eval_macro_f1']\n",
    "    val_accuracy = results['eval_accuracy']\n",
    "    print(f\"Validation - Macro F1: {val_f1:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    val_preds = trainer_single.predict(val_dataset_single)\n",
    "    val_probs = torch.softmax(torch.tensor(val_preds.predictions), dim=1).numpy()[:, 1]\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "        preds = (val_probs > threshold).astype(int)\n",
    "        f1 = f1_score(val_labels_single, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Optimal threshold: {best_threshold:.2f} (F1: {best_f1:.4f})\")\n",
    "    \n",
    "    ensemble_trainers.append(trainer_single)\n",
    "    ensemble_models.append(model_single)\n",
    "    ensemble_thresholds.append(best_threshold)\n",
    "    ensemble_f1_scores.append(best_f1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Binary Ensemble Training Complete\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf1ce5",
   "metadata": {},
   "source": [
    "### 8.2 Ensemble Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a60a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ensemble predictions on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binary Ensemble Results\n",
      "  Macro F1:  0.4563\n",
      "  Accuracy:  0.0269\n",
      "  F1 Gain:   +0.1016\n",
      "\n",
      "Per-label F1 scores:\n",
      "  political: 0.7099\n",
      "  racial/ethnic: 0.5479\n",
      "  religious: 0.5556\n",
      "  gender/sexual: 0.3810\n",
      "  other: 0.0870\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating ensemble predictions on validation set...\")\n",
    "\n",
    "ensemble_val_predictions = np.zeros((len(val_labels_ml), 5), dtype=int)\n",
    "\n",
    "for i, (trainer, threshold, label_name) in enumerate(zip(ensemble_trainers, ensemble_thresholds, label_columns)):\n",
    "    val_dataset_single = PolarizationDataset(val_texts_ml, val_labels_ml[:, i], tokenizer_multilabel)\n",
    "    preds = trainer.predict(val_dataset_single)\n",
    "    probs = torch.softmax(torch.tensor(preds.predictions), dim=1).numpy()[:, 1]\n",
    "    ensemble_val_predictions[:, i] = (probs > threshold).astype(int)\n",
    "\n",
    "# Calculate ensemble metrics\n",
    "ensemble_macro_f1 = f1_score(val_labels_ml, ensemble_val_predictions, average='macro', zero_division=0)\n",
    "ensemble_accuracy = accuracy_score(val_labels_ml, ensemble_val_predictions)\n",
    "per_label_f1 = f1_score(val_labels_ml, ensemble_val_predictions, average=None, zero_division=0)\n",
    "\n",
    "print(\"\\nBinary Ensemble Results\")\n",
    "print(f\"  Macro F1:  {ensemble_macro_f1:.4f}\")\n",
    "print(f\"  Accuracy:  {ensemble_accuracy:.4f}\")\n",
    "print(f\"  F1 Gain:   +{(ensemble_macro_f1 - eval_results_ml['eval_macro_f1']):.4f}\")\n",
    "\n",
    "print(\"\\nPer-label F1 scores:\")\n",
    "for i, label in enumerate(label_columns):\n",
    "    print(f\"  {label}: {per_label_f1[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb0e1b",
   "metadata": {},
   "source": [
    "### 8.3 Generate Ensemble Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a22ecf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ensemble test predictions...\n",
      "  Predicting political...\n",
      "  Predicting racial/ethnic...\n",
      "  Predicting religious...\n",
      "  Predicting gender/sexual...\n",
      "  Predicting other...\n",
      "\n",
      "Ensemble test predictions:\n",
      "  political: 51\n",
      "  racial/ethnic: 12\n",
      "  religious: 5\n",
      "  gender/sexual: 3\n",
      "  other: 52\n",
      "\n",
      "Saved: submission_subtask2_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating ensemble test predictions...\")\n",
    "\n",
    "ensemble_test_predictions = np.zeros((len(polarized_texts), 5), dtype=int)\n",
    "\n",
    "for i, (model, threshold, label_name) in enumerate(zip(ensemble_models, ensemble_thresholds, label_columns)):\n",
    "    print(f\"  Predicting {label_name}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j in range(0, len(polarized_texts), 32):\n",
    "            batch_texts = polarized_texts[j:j+32]\n",
    "            encodings = tokenizer_multilabel(\n",
    "                list(batch_texts),\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "            \n",
    "            outputs = model(**encodings)\n",
    "            probs = torch.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    all_probs = np.array(all_probs)\n",
    "    ensemble_test_predictions[:, i] = (all_probs > threshold).astype(int)\n",
    "\n",
    "# Create full predictions\n",
    "full_ensemble_predictions = np.zeros((len(test_texts), 5), dtype=int)\n",
    "full_ensemble_predictions[polarized_indices] = ensemble_test_predictions\n",
    "\n",
    "print(\"\\nEnsemble test predictions:\")\n",
    "for i, col in enumerate(label_columns):\n",
    "    print(f\"  {col}: {full_ensemble_predictions[:, i].sum()}\")\n",
    "\n",
    "# Save ensemble submission\n",
    "df_submission_ensemble = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'political': full_ensemble_predictions[:, 0],\n",
    "    'racial/ethnic': full_ensemble_predictions[:, 1],\n",
    "    'religious': full_ensemble_predictions[:, 2],\n",
    "    'gender/sexual': full_ensemble_predictions[:, 3],\n",
    "    'other': full_ensemble_predictions[:, 4]\n",
    "})\n",
    "df_submission_ensemble.to_csv('submission_subtask2_ensemble.csv', index=False)\n",
    "\n",
    "print(\"\\nSaved: submission_subtask2_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae6986c",
   "metadata": {},
   "source": [
    "## 9. Alternative: Class-Weighted Model Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711daf79",
   "metadata": {},
   "source": [
    "### 9.1 Test Predictions with Class-Weighted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ad7b0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarized texts: 52\n",
      "\n",
      "Test predictions:\n",
      "  political: 52\n",
      "  racial/ethnic: 13\n",
      "  religious: 5\n",
      "  gender/sexual: 2\n",
      "  other: 20\n"
     ]
    }
   ],
   "source": [
    "# Use weighted model with optimized thresholds (best performance)\n",
    "polarized_indices = np.where(test_pred_labels == 1)[0]\n",
    "polarized_texts = test_texts[polarized_indices]\n",
    "\n",
    "test_dataset_ml = MultiLabelDataset(\n",
    "    polarized_texts,\n",
    "    np.zeros((len(polarized_texts), 5)),\n",
    "    tokenizer_multilabel\n",
    ")\n",
    "\n",
    "test_predictions_ml = trainer_weighted.predict(test_dataset_ml)\n",
    "test_pred_probs_ml = torch.sigmoid(torch.tensor(test_predictions_ml.predictions)).numpy()\n",
    "\n",
    "test_pred_labels_ml = np.zeros_like(test_pred_probs_ml, dtype=int)\n",
    "for i, threshold in enumerate(optimal_thresholds_weighted):\n",
    "    test_pred_labels_ml[:, i] = (test_pred_probs_ml[:, i] > threshold).astype(int)\n",
    "\n",
    "full_predictions = np.zeros((len(test_texts), 5), dtype=int)\n",
    "full_predictions[polarized_indices] = test_pred_labels_ml\n",
    "\n",
    "print(f\"Polarized texts: {len(polarized_indices)}\")\n",
    "print(\"\\nTest predictions:\")\n",
    "for i, col in enumerate(label_columns):\n",
    "    print(f\"  {col}: {full_predictions[:, i].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e337e",
   "metadata": {},
   "source": [
    "### 9.2 Save Class-Weighted Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97781748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_subtask2.csv\n"
     ]
    }
   ],
   "source": [
    "df_submission_task2 = pd.DataFrame({\n",
    "    'id': df_test['id'],\n",
    "    'political': full_predictions[:, 0],\n",
    "    'racial/ethnic': full_predictions[:, 1],\n",
    "    'religious': full_predictions[:, 2],\n",
    "    'gender/sexual': full_predictions[:, 3],\n",
    "    'other': full_predictions[:, 4]\n",
    "})\n",
    "df_submission_task2.to_csv('submission_subtask2.csv', index=False)\n",
    "\n",
    "print(\"Saved: submission_subtask2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f34736",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c31d5dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Subtask 1: Binary Classification\n",
      "  Macro F1:  0.8299\n",
      "  Accuracy:  0.8430\n",
      "\n",
      "Subtask 2: Multi-Label Classification\n",
      "  Baseline Model:\n",
      "    Macro F1:  0.3546\n",
      "    Accuracy:  0.7169\n",
      "\n",
      "  Class-Weighted Model:\n",
      "    Macro F1:  0.4247\n",
      "    Accuracy:  0.6860\n",
      "    F1 Gain:   +0.0700\n",
      "\n",
      "  Baseline + Threshold Tuning:\n",
      "    Macro F1:  0.4394\n",
      "    Accuracy:  0.6674\n",
      "    F1 Gain:   +0.0847\n",
      "\n",
      "  Class-Weighted + Threshold Tuning:\n",
      "    Macro F1:  0.4938\n",
      "    Accuracy:  0.6860\n",
      "    F1 Gain:   +0.1391\n",
      "\n",
      "  Binary Ensemble (Best):\n",
      "    Macro F1:  0.4563\n",
      "    Accuracy:  0.0269\n",
      "    F1 Gain:   +0.1016\n",
      "\n",
      "Submission Files:\n",
      "  - submission_subtask1.csv\n",
      "  - submission_subtask2.csv (class-weighted + threshold tuning)\n",
      "  - submission_subtask2_ensemble.csv (binary ensemble - recommended)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nSubtask 1: Binary Classification\")\n",
    "print(f\"  Macro F1:  {eval_results['eval_macro_f1']:.4f}\")\n",
    "print(f\"  Accuracy:  {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nSubtask 2: Multi-Label Classification\")\n",
    "print(f\"  Baseline Model:\")\n",
    "print(f\"    Macro F1:  {eval_results_ml['eval_macro_f1']:.4f}\")\n",
    "print(f\"    Accuracy:  {eval_results_ml['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Class-Weighted Model:\")\n",
    "print(f\"    Macro F1:  {eval_results_weighted['eval_macro_f1']:.4f}\")\n",
    "print(f\"    Accuracy:  {eval_results_weighted['eval_accuracy']:.4f}\")\n",
    "print(f\"    F1 Gain:   +{(eval_results_weighted['eval_macro_f1'] - eval_results_ml['eval_macro_f1']):.4f}\")\n",
    "\n",
    "print(f\"\\n  Baseline + Threshold Tuning:\")\n",
    "print(f\"    Macro F1:  {macro_f1_optimized:.4f}\")\n",
    "print(f\"    Accuracy:  {accuracy_optimized:.4f}\")\n",
    "print(f\"    F1 Gain:   +{(macro_f1_optimized - eval_results_ml['eval_macro_f1']):.4f}\")\n",
    "\n",
    "print(f\"\\n  Class-Weighted + Threshold Tuning:\")\n",
    "print(f\"    Macro F1:  {macro_f1_weighted_optimized:.4f}\")\n",
    "print(f\"    Accuracy:  {accuracy_weighted_optimized:.4f}\")\n",
    "print(f\"    F1 Gain:   +{(macro_f1_weighted_optimized - eval_results_ml['eval_macro_f1']):.4f}\")\n",
    "\n",
    "print(f\"\\n  Binary Ensemble (Best):\")\n",
    "print(f\"    Macro F1:  {ensemble_macro_f1:.4f}\")\n",
    "print(f\"    Accuracy:  {ensemble_accuracy:.4f}\")\n",
    "print(f\"    F1 Gain:   +{(ensemble_macro_f1 - eval_results_ml['eval_macro_f1']):.4f}\")\n",
    "\n",
    "print(\"\\nSubmission Files:\")\n",
    "print(\"  - submission_subtask1.csv\")\n",
    "print(\"  - submission_subtask2.csv (class-weighted + threshold tuning)\")\n",
    "print(\"  - submission_subtask2_ensemble.csv (binary ensemble - recommended)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2767a",
   "metadata": {},
   "source": [
    "## 11. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08cd04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save trained models\n",
    "model_binary.save_pretrained('./saved_models/binary_classifier')\n",
    "tokenizer_binary.save_pretrained('./saved_models/binary_classifier')\n",
    "\n",
    "model_multilabel_weighted.save_pretrained('./saved_models/multilabel_classifier')\n",
    "tokenizer_multilabel.save_pretrained('./saved_models/multilabel_classifier')\n",
    "\n",
    "# Save ensemble models\n",
    "for i, (model, label) in enumerate(zip(ensemble_models, label_columns)):\n",
    "    save_path = f'./saved_models/ensemble_{label.replace(\"/\", \"_\")}'\n",
    "    model.save_pretrained(save_path)\n",
    "\n",
    "print(\"Models saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
